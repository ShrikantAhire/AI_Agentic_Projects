from llama_index.llms.google_genai import GoogleGenAI
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

# Initialize the Google GenAI LLM
llm = GoogleGenAI(
    model=os.getenv("GEMINI_MODEL"),
    api_key=os.getenv("GOOGLE_API_KEY")
)

# Load the documents from the specified directory
reader = SimpleDirectoryReader(input_dir="./data")
documents = reader.load_data()

# Initialize the embedding model
Settings.embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5"
)
# Create a vector store index from the loaded documents
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine(llm=llm)

# Ask a question
response1 = query_engine.query("What are the names of resume candidates?")
print("\nðŸ“„ Answer 1:\n", response1)

response2 = query_engine.query("What is Caleb Foster designation?")
print("\nðŸ“„ Answer 2:\n", response2)

response3 = query_engine.query("What are the key skills of these resumes?")
print("\nðŸ“„ Answer 3:\n", response3)
